{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM\n",
    "    )\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Literal,Annotated\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stored_result(path:str, type_:Literal[\"json\",\"csv\"]):\n",
    "    # Check if the path is valid\n",
    "    if not os.path.exists(path):\n",
    "        print(\"The path is not valid or the file does not exist\")\n",
    "        return None \n",
    "    if type_ == \"json\":\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    elif type_ == \"csv\":\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Invalid type\")\n",
    "        return None \n",
    "    \n",
    "def save_result(result, path:str, type_:Literal[\"json\",\"csv\"], indent:int=None):\n",
    "    if type_ == \"json\":\n",
    "        with open(path, 'w') as f:\n",
    "            if indent is not None:\n",
    "                json.dump(result, f, indent=indent)\n",
    "            else:\n",
    "                json.dump(result, f)\n",
    "    elif type_ == \"csv\":\n",
    "        #  check if the result is a dataframe\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result.to_csv(path, index=False)\n",
    "        else:\n",
    "            print(\"The result is not a dataframe\")\n",
    "            raise ValueError(\"The result is not a dataframe\")\n",
    "    else:\n",
    "        print(\"Invalid type\")\n",
    "        raise ValueError(\"Invalid type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splade Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseModel:\n",
    "  def __init__(self,max_length=512):\n",
    "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-cocondenser-selfdistil\").to(self.device)\n",
    "    self.max_length = max_length\n",
    "    self.sparse_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-selfdistil\",truncation=True, padding='max_length', max_length=self.max_length)\n",
    "    \n",
    "\n",
    "  def decode_sparse_dict(self, sparse_dict,trim=None):\n",
    "    a = np.zeros((30522))\n",
    "    a[sparse_dict['indices']] = sparse_dict['values']\n",
    "    if trim is not None:\n",
    "      a[a.argsort()[:-trim]] = 0\n",
    "    return a\n",
    "\n",
    "  def decode_sparse_dicts(self, sparse_dicts,trim=None):\n",
    "    res = []\n",
    "    for _ in sparse_dicts:\n",
    "      res.append(self.decode_sparse_dict(_,trim).tolist())\n",
    "    return res\n",
    "\n",
    "  def formalize(self, sparse_dict):\n",
    "    idx2token = {idx: token for token, idx in self.sparse_tokenizer.get_vocab().items()}\n",
    "    sparse_dict_tokens = {\n",
    "        idx2token[idx]: weight for idx, weight in zip(sparse_dict['indices'], sparse_dict['values'])\n",
    "    }\n",
    "    sparse_dict_tokens = {\n",
    "        k: v for k, v in sorted(\n",
    "            sparse_dict_tokens.items(),\n",
    "            key=lambda item: item[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    }\n",
    "    return sparse_dict_tokens\n",
    "\n",
    "  # This function is used to encode a list of texts into sparse vectors. Process all texts at once\n",
    "  # Use this function if you have a GPU. Padding is used to make all texts have the same length\n",
    "  # Faster in GPU , Slower in CPU\n",
    "  def encode_texts(self, texts:List[str]):\n",
    "    input_ids = self.sparse_tokenizer(texts, return_tensors='pt', padding='max_length' if len(texts)>1 else False).to(self.device)\n",
    "    input_ids = {k: v[:self.max_length] for k, v in input_ids.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits = self.model(**input_ids).logits\n",
    "\n",
    "    sparse_vecs = torch.max(\n",
    "        torch.log(\n",
    "            1+torch.relu(logits)\n",
    "        )*input_ids['attention_mask'].unsqueeze(-1),\n",
    "    dim=1)[0].cpu()\n",
    "\n",
    "    sparse_dicts = []\n",
    "    for sparse_vec in sparse_vecs:\n",
    "      indices = sparse_vec.nonzero().squeeze().tolist()\n",
    "      values = sparse_vec[indices].tolist()\n",
    "      sparse_dict = {'indices': indices, 'values': values}\n",
    "      sparse_dicts.append(sparse_dict)\n",
    "\n",
    "    return sparse_dicts\n",
    "  \n",
    "  # This function is used to encode a single text into a sparse vector\n",
    "  def encode_text(self, text:str):\n",
    "    return self.encode_texts([text])[0]\n",
    "\n",
    "  # This function is used to encode a list of texts into sparse vectors (It iterates over one text at a time)\n",
    "  # Faster in CPU , Slower in GPU. Maintain batch_size=1\n",
    "  def encode_text_list(self, texts:list, batch_size:int=1):\n",
    "\n",
    "    sparse_dicts = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "      batch = texts[i:i+batch_size]\n",
    "      sparse_dicts += self.encode_texts(batch)\n",
    "\n",
    "    return sparse_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spalde Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splade_model = SparseModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json_data = get_stored_result(\"./sample-data/data1.json\", \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time for processing a text at a time: 11.123236179351807\n"
     ]
    }
   ],
   "source": [
    "## Elpased Time Calulation for processing a text at a time\n",
    "encoded_result = {}\n",
    "start = time.time()\n",
    "for idx, metadata in sample_json_data.items():\n",
    "    encoded_result[idx] = splade_model.encode_text(metadata['content'])\n",
    "    encoded_result[idx]['metadata'] = metadata\n",
    "end = time.time()\n",
    "print(f\"Elapsed Time for processing a text at a time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time for processing a list of texts: 7.633612155914307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Elpased Time Calulation for processing a list of texts\n",
    "encoded_result = {}\n",
    "start = time.time()\n",
    "texts = [metadata['content'] for metadata in sample_json_data.values()]\n",
    "encoded_result = splade_model.encode_text_list(texts)\n",
    "end = time.time()\n",
    "print(f\"Elapsed Time for processing a list of texts: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Elpased Time Calulation for processing all texts at once\n",
    "# encoded_result = {}\n",
    "# start = time.time()\n",
    "# texts = [metadata['content'] for metadata in sample_json_data.values()]\n",
    "# results = splade_model.encode_texts(texts)\n",
    "# end = time.time()\n",
    "# print(f\"Elapsed Time for processing all texts at once: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(encoded_result, \"./sample-data/encoded_data1.json\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_community.retrievers import QdrantSparseVectorRetriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCLIENT = QdrantClient(path=\"./.qdrant-vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantHandler:\n",
    "    def __init__(self, qclient, **kwargs):\n",
    "        self.qclient = qclient\n",
    "        self.retreivers = {}\n",
    "        self.sparse_enocder = kwargs.get(\"sparse_enocder\", None)\n",
    "    \n",
    "    def init_collections(self, collection_names:List[str],reset_before_init:bool=False):\n",
    "        for collection_name in collection_names:\n",
    "            if reset_before_init:\n",
    "                self.qclient.delete_collection(collection_name)\n",
    "            try:\n",
    "                if not self.qclient.collection_exists(collection_name):\n",
    "                    self.qclient.create_collection(\n",
    "                        collection_name,\n",
    "                        vectors_config={},\n",
    "                        sparse_vectors_config={\n",
    "                            \"sparse_vector\": models.SparseVectorParams(\n",
    "                                index=models.SparseIndexParams(\n",
    "                                    on_disk=False,\n",
    "                                )\n",
    "                            )\n",
    "                        },\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Collection {collection_name} already exists. Reusing the collection\")\n",
    "\n",
    "            print(f\"Collection {collection_name} is created\")\n",
    "            retriever = QdrantSparseVectorRetriever(\n",
    "                client=self.qclient,\n",
    "                collection_name=collection_name,\n",
    "                sparse_vector_name=\"sparse_vector\",\n",
    "                sparse_encoder=self.sparse_enocder\n",
    "            )\n",
    "\n",
    "            self.retreivers[collection_name] = retriever\n",
    "\n",
    "    def add_documents(self, collection_name:str, data:List[Dict[str, Any]], pk:str=\"id\",content_key:str=\"content\", batch_size:int=10):\n",
    "        \n",
    "        document_list = [ \n",
    "            Document(\n",
    "                metadata = doc,\n",
    "                page_content = doc[content_key]\n",
    "            ) for doc in data\n",
    "        ]\n",
    "\n",
    "        id_list = [doc[pk] for doc in data]\n",
    "\n",
    "        for i in tqdm(range(0, len(document_list), batch_size)):\n",
    "            batch = document_list[i:i+batch_size]\n",
    "            batch_ids = id_list[i:i+batch_size]\n",
    "            self.retreivers[collection_name].add_documents(documents=batch, ids=batch_ids)\n",
    "    \n",
    "    def retrieve(self, collection_name:str, query:str, top_k:int=10):\n",
    "        return self.retreivers[collection_name].invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HANDLER = QdrantHandler(QCLIENT, sparse_enocder=lambda x: tuple(splade_model.encode_text(x).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection sample_collection is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/w9d_8wb17m50mtkv2w368bqc0000gn/T/ipykernel_19483/701227903.py:28: LangChainDeprecationWarning: The class `QdrantSparseVectorRetriever` was deprecated in LangChain 0.2.16 and will be removed in 0.5.0. Use :meth:`~Qdrant vector store now supports sparse retrievals natively. Use langchain_qdrant.QdrantVectorStore#as_retriever() instead. Reference: https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#sparse-vector-search` instead.\n",
      "  retriever = QdrantSparseVectorRetriever(\n"
     ]
    }
   ],
   "source": [
    "QDRANT_HANDLER.init_collections([\"sample_collection\"], reset_before_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDRANT_HANDLER.add_documents(\"sample_collection\", list(sample_json_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = QDRANT_HANDLER.retrieve(\"sample_collection\", \"What is nerural network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A neural network consists of layers of interconnected nodes. Each node in a layer processes an input, applies a weight and an activation function, and passes the result to the next layer. The final output layer generates predictions based on the input data.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,ToolMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from together import Together\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_generator(query: str, contexts: List[str], history: List[Any]):\n",
    "    # Define the system prompt for concise question answering\n",
    "    system_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a template with placeholders for system prompt, query, context, and history\n",
    "    template = \"\"\"\n",
    "    System Instruction:\n",
    "    {system_prompt}\n",
    "\n",
    "    Given the following context:\n",
    "    {contexts}\n",
    "\n",
    "    And the previous conversation history:\n",
    "    {history}\n",
    "\n",
    "    Answer the following query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the inputs into a prompt template using Langchain's PromptTemplate class\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"contexts\", \"history\", \"query\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    history_text = \"\\n\".join([f\"{'Human' if isinstance(item, HumanMessage) else 'System'}: {item.content}\" for item in history]) \n",
    "\n",
    "    # Format the prompt with the actual values\n",
    "    formatted_prompt = prompt.format(\n",
    "        system_prompt=system_prompt.strip(),\n",
    "        contexts=\"\\n\".join(contexts),\n",
    "        history=history_text,\n",
    "        query=query\n",
    "    )\n",
    "    \n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEngine:\n",
    "    def __init__(self, model_path:str=\"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\"):\n",
    "        self.client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))\n",
    "        self.model_path = model_path\n",
    "    \n",
    "    def invoke(self, query:str, contexts:List[str], history:List[Any]):\n",
    "        prompt=prompt_generator(\n",
    "                query=query,\n",
    "                contexts=contexts,\n",
    "                history=history\n",
    "                )\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_path,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ENGINE = LLMEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    cur_state: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "    contexts: List[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentFunctions:\n",
    "    \n",
    "    config = {\n",
    "        \"qdrant_handler\": QDRANT_HANDLER,\n",
    "        \"sample_collection\": \"sample_collection\"\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def welcome(state:AgentState)->AgentState:\n",
    "        msg = SystemMessage(\"Welcome to the system. How can I help you?\")\n",
    "        state['cur_state'] = \"welcome\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        state['messages'].append(msg)\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def human_input(state:AgentState)->AgentState:\n",
    "        state['cur_state'] = \"human_input\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def retreive_documents(state:AgentState)->AgentState:\n",
    "        query = state['messages'][-1].content\n",
    "        retrieved_nodes = AgentFunctions[\"qdrant_handler\"].retrieve(AgentFunctions[\"sample_collection\"], query)\n",
    "        state['contexts'] = [node.page_content for node in retrieved_nodes] \n",
    "        state['cur_state'] = \"retrieval\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def invoke_llm(state:AgentState)->AgentState:\n",
    "        query = state['messages'][-1].content\n",
    "        history = state['messages'][-4:-1]\n",
    "        contexts = state['contexts']\n",
    "        response = LLM_ENGINE.invoke(query, contexts, history)\n",
    "        msg = SystemMessage(response)\n",
    "        state['messages'].append(msg)\n",
    "        state['cur_state'] = \"invoke_llm\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_end_of_conversation(state:AgentState)->bool:\n",
    "        state['cur_state'] = \"check_end_of_conversation\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        if state['messages'][-1].content == \"/quit\":\n",
    "            return \"quit\"\n",
    "        return \"continue\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def exit_conversation(state:AgentState)->AgentState:\n",
    "        state['cur_state'] = \"exit_conversation\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        msg = SystemMessage(\"Goodbye!\")\n",
    "        state['messages'].append(msg)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent:\n",
    "    def __init__(self,thread_id):\n",
    "        self.config = None\n",
    "        self.app = None\n",
    "        self.build(thread_id)\n",
    "\n",
    "    def draw_workflow(self):\n",
    "        display(Image(self.app.get_graph(xray=True).draw_mermaid_png()))\n",
    "    \n",
    "    def build(self, thread_id):\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"welcome\", AgentFunctions.welcome)\n",
    "        workflow.add_node(\"human_input\", AgentFunctions.human_input)\n",
    "        workflow.add_node(\"retreive_documents\", AgentFunctions.retreive_documents)\n",
    "        workflow.add_node(\"invoke_llm\", AgentFunctions.invoke_llm)\n",
    "        workflow.add_node(\"exit_conversation\", AgentFunctions.exit_conversation)\n",
    "\n",
    "        \n",
    "        workflow.add_edge(\"welcome\", \"human_input\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"human_input\", \n",
    "            AgentFunctions.check_end_of_conversation,\n",
    "            {\n",
    "                \"quit\": \"exit_conversation\",\n",
    "                \"continue\": \"retreive_documents\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"retreive_documents\", \"invoke_llm\")\n",
    "        workflow.add_edge(\"invoke_llm\", \"human_input\")\n",
    "        workflow.add_edge(\"exit_conversation\", langgraph.graph.END)\n",
    "\n",
    "        workflow.set_entry_point(\"welcome\")\n",
    "        \n",
    "        memory = MemorySaver()\n",
    "\n",
    "        app = workflow.compile(\n",
    "            checkpointer=memory,\n",
    "            interrupt_before=[\"human_input\"],\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Assign the compiled app before the connection closes\n",
    "        self.app = app\n",
    "        self.config = {\"configurable\": {\"thread_id\": str(thread_id)}}\n",
    "\n",
    "    def get_recent_state_snap(self):\n",
    "        return self.app.get_state(config=self.config).values.copy()\n",
    "\n",
    "    def get_last_message(self):\n",
    "        snap = self.get_recent_state_snap()\n",
    "        return snap[\"messages\"][-1]\n",
    "    \n",
    "    def continue_flow(self, state):\n",
    "        self.app.invoke(state, config=self.config)\n",
    "        return self.get_recent_state_snap()\n",
    "    \n",
    "    def resume_with_user_input(self, user_input:str):\n",
    "        snap = self.get_recent_state_snap()\n",
    "        snap[\"messages\"].append(HumanMessage(user_input))\n",
    "        self.app.update_state(self.config, snap)\n",
    "        return self.continue_flow(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_agent = MyAgent(thread_id=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_agent.draw_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: welcome\n"
     ]
    }
   ],
   "source": [
    "sample_snap0 =  sample_agent.continue_flow({\n",
    "    \"cur_state\": \"start\",\n",
    "    \"messages\": [],\n",
    "    \"contexts\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the system. How can I help you?\n"
     ]
    }
   ],
   "source": [
    "print(sample_agent.get_last_message().content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cur_state': 'welcome',\n",
       " 'messages': [SystemMessage(content='Welcome to the system. How can I help you?', additional_kwargs={}, response_metadata={}, id='de751a8f-fe34-4fa6-8656-7df2147e17aa'),\n",
       "  HumanMessage(content='Hi! I want to know about neural networks', additional_kwargs={}, response_metadata={}, id='b354214e-e4df-4a9b-8c91-96e05cba83c8')],\n",
       " 'contexts': []}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_agent.get_recent_state_snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: human_input\n",
      "Current State: check_end_of_conversation\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_snap1 \u001b[38;5;241m=\u001b[39m \u001b[43msample_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_with_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi! I want to know about neural networks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 61\u001b[0m, in \u001b[0;36mMyAgent.resume_with_user_input\u001b[0;34m(self, user_input)\u001b[0m\n\u001b[1;32m     59\u001b[0m snap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(HumanMessage(user_input))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mupdate_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, snap)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinue_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 54\u001b[0m, in \u001b[0;36mMyAgent.continue_flow\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinue_flow\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_recent_state_snap()\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1545\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1547\u001b[0m     config,\n\u001b[1;32m   1548\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1549\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1550\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1551\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1552\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1554\u001b[0m ):\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1556\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1278\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1273\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1274\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1275\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1276\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1277\u001b[0m     ):\n\u001b[0;32m-> 1278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1279\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1280\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1281\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1282\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1283\u001b[0m         ):\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/runner.py:52\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     50\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/utils/runnable.py:387\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/utils/runnable.py:167\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 167\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:95\u001b[0m, in \u001b[0;36mBranch._route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     93\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     94\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:130\u001b[0m, in \u001b[0;36mBranch._finish\u001b[0;34m(self, writer, input, result, config)\u001b[0m\n\u001b[1;32m    128\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[0;32m--> 130\u001b[0m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    132\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m    130\u001b[0m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 131\u001b[0m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    132\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "sample_snap1 = sample_agent.resume_with_user_input(\"Hi! I want to know about neural networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: human_input\n",
      "Current State: check_end_of_conversation\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1545\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1547\u001b[0m     config,\n\u001b[1;32m   1548\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1549\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1550\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1551\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1552\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1554\u001b[0m ):\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1556\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1278\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1273\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1274\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1275\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1276\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1277\u001b[0m     ):\n\u001b[0;32m-> 1278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1279\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1280\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1281\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1282\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1283\u001b[0m         ):\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/runner.py:52\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     50\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/utils/runnable.py:387\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/utils/runnable.py:167\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 167\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:95\u001b[0m, in \u001b[0;36mBranch._route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     93\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     94\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:130\u001b[0m, in \u001b[0;36mBranch._finish\u001b[0;34m(self, writer, input, result, config)\u001b[0m\n\u001b[1;32m    128\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[0;32m--> 130\u001b[0m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    132\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "File \u001b[0;32m~/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/langgraph/graph/graph.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m    130\u001b[0m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 131\u001b[0m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    132\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "sample_agent.app.invoke(None, config=sample_agent.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM\n",
    "    )\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Literal,Annotated\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stored_result(path:str, type_:Literal[\"json\",\"csv\"]):\n",
    "    # Check if the path is valid\n",
    "    if not os.path.exists(path):\n",
    "        print(\"The path is not valid or the file does not exist\")\n",
    "        return None \n",
    "    if type_ == \"json\":\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    elif type_ == \"csv\":\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Invalid type\")\n",
    "        return None \n",
    "    \n",
    "def save_result(result, path:str, type_:Literal[\"json\",\"csv\"], indent:int=None):\n",
    "    if type_ == \"json\":\n",
    "        with open(path, 'w') as f:\n",
    "            if indent is not None:\n",
    "                json.dump(result, f, indent=indent)\n",
    "            else:\n",
    "                json.dump(result, f)\n",
    "    elif type_ == \"csv\":\n",
    "        #  check if the result is a dataframe\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result.to_csv(path, index=False)\n",
    "        else:\n",
    "            print(\"The result is not a dataframe\")\n",
    "            raise ValueError(\"The result is not a dataframe\")\n",
    "    else:\n",
    "        print(\"Invalid type\")\n",
    "        raise ValueError(\"Invalid type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splade Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseModel:\n",
    "  def __init__(self,max_length=512):\n",
    "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-cocondenser-selfdistil\").to(self.device)\n",
    "    self.max_length = max_length\n",
    "    self.sparse_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-selfdistil\",truncation=True, padding='max_length', max_length=self.max_length)\n",
    "    \n",
    "\n",
    "  def decode_sparse_dict(self, sparse_dict,trim=None):\n",
    "    a = np.zeros((30522))\n",
    "    a[sparse_dict['indices']] = sparse_dict['values']\n",
    "    if trim is not None:\n",
    "      a[a.argsort()[:-trim]] = 0\n",
    "    return a\n",
    "\n",
    "  def decode_sparse_dicts(self, sparse_dicts,trim=None):\n",
    "    res = []\n",
    "    for _ in sparse_dicts:\n",
    "      res.append(self.decode_sparse_dict(_,trim).tolist())\n",
    "    return res\n",
    "\n",
    "  def formalize(self, sparse_dict):\n",
    "    idx2token = {idx: token for token, idx in self.sparse_tokenizer.get_vocab().items()}\n",
    "    sparse_dict_tokens = {\n",
    "        idx2token[idx]: weight for idx, weight in zip(sparse_dict['indices'], sparse_dict['values'])\n",
    "    }\n",
    "    sparse_dict_tokens = {\n",
    "        k: v for k, v in sorted(\n",
    "            sparse_dict_tokens.items(),\n",
    "            key=lambda item: item[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    }\n",
    "    return sparse_dict_tokens\n",
    "\n",
    "  # This function is used to encode a list of texts into sparse vectors. Process all texts at once\n",
    "  # Use this function if you have a GPU. Padding is used to make all texts have the same length\n",
    "  # Faster in GPU , Slower in CPU\n",
    "  def encode_texts(self, texts:List[str]):\n",
    "    input_ids = self.sparse_tokenizer(texts, return_tensors='pt', padding='max_length' if len(texts)>1 else False).to(self.device)\n",
    "    input_ids = {k: v[:self.max_length] for k, v in input_ids.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits = self.model(**input_ids).logits\n",
    "\n",
    "    sparse_vecs = torch.max(\n",
    "        torch.log(\n",
    "            1+torch.relu(logits)\n",
    "        )*input_ids['attention_mask'].unsqueeze(-1),\n",
    "    dim=1)[0].cpu()\n",
    "\n",
    "    sparse_dicts = []\n",
    "    for sparse_vec in sparse_vecs:\n",
    "      indices = sparse_vec.nonzero().squeeze().tolist()\n",
    "      values = sparse_vec[indices].tolist()\n",
    "      sparse_dict = {'indices': indices, 'values': values}\n",
    "      sparse_dicts.append(sparse_dict)\n",
    "\n",
    "    return sparse_dicts\n",
    "  \n",
    "  # This function is used to encode a single text into a sparse vector\n",
    "  def encode_text(self, text:str):\n",
    "    return self.encode_texts([text])[0]\n",
    "\n",
    "  # This function is used to encode a list of texts into sparse vectors (It iterates over one text at a time)\n",
    "  # Faster in CPU , Slower in GPU. Maintain batch_size=1\n",
    "  def encode_text_list(self, texts:list, batch_size:int=1):\n",
    "\n",
    "    sparse_dicts = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "      batch = texts[i:i+batch_size]\n",
    "      sparse_dicts += self.encode_texts(batch)\n",
    "\n",
    "    return sparse_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spalde Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/llm-helper/.myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splade_model = SparseModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json_data = get_stored_result(\"./sample-data/data1.json\", \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time for processing a text at a time: 11.123236179351807\n"
     ]
    }
   ],
   "source": [
    "## Elpased Time Calulation for processing a text at a time\n",
    "encoded_result = {}\n",
    "start = time.time()\n",
    "for idx, metadata in sample_json_data.items():\n",
    "    encoded_result[idx] = splade_model.encode_text(metadata['content'])\n",
    "    encoded_result[idx]['metadata'] = metadata\n",
    "end = time.time()\n",
    "print(f\"Elapsed Time for processing a text at a time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time for processing a list of texts: 7.633612155914307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Elpased Time Calulation for processing a list of texts\n",
    "encoded_result = {}\n",
    "start = time.time()\n",
    "texts = [metadata['content'] for metadata in sample_json_data.values()]\n",
    "encoded_result = splade_model.encode_text_list(texts)\n",
    "end = time.time()\n",
    "print(f\"Elapsed Time for processing a list of texts: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Elpased Time Calulation for processing all texts at once\n",
    "# encoded_result = {}\n",
    "# start = time.time()\n",
    "# texts = [metadata['content'] for metadata in sample_json_data.values()]\n",
    "# results = splade_model.encode_texts(texts)\n",
    "# end = time.time()\n",
    "# print(f\"Elapsed Time for processing all texts at once: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(encoded_result, \"./sample-data/encoded_data1.json\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_community.retrievers import QdrantSparseVectorRetriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCLIENT = QdrantClient(path=\"./.qdrant-vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantHandler:\n",
    "    def __init__(self, qclient, **kwargs):\n",
    "        self.qclient = qclient\n",
    "        self.retreivers = {}\n",
    "        self.sparse_enocder = kwargs.get(\"sparse_enocder\", None)\n",
    "    \n",
    "    def init_collections(self, collection_names:List[str],reset_before_init:bool=False):\n",
    "        for collection_name in collection_names:\n",
    "            if reset_before_init:\n",
    "                self.qclient.delete_collection(collection_name)\n",
    "            try:\n",
    "                if not self.qclient.collection_exists(collection_name):\n",
    "                    self.qclient.create_collection(\n",
    "                        collection_name,\n",
    "                        vectors_config={},\n",
    "                        sparse_vectors_config={\n",
    "                            \"sparse_vector\": models.SparseVectorParams(\n",
    "                                index=models.SparseIndexParams(\n",
    "                                    on_disk=False,\n",
    "                                )\n",
    "                            )\n",
    "                        },\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Collection {collection_name} already exists. Reusing the collection\")\n",
    "\n",
    "            print(f\"Collection {collection_name} is created\")\n",
    "            retriever = QdrantSparseVectorRetriever(\n",
    "                client=self.qclient,\n",
    "                collection_name=collection_name,\n",
    "                sparse_vector_name=\"sparse_vector\",\n",
    "                sparse_encoder=self.sparse_enocder\n",
    "            )\n",
    "\n",
    "            self.retreivers[collection_name] = retriever\n",
    "\n",
    "    def add_documents(self, collection_name:str, data:List[Dict[str, Any]], pk:str=\"id\",content_key:str=\"content\", batch_size:int=10):\n",
    "        \n",
    "        document_list = [ \n",
    "            Document(\n",
    "                metadata = doc,\n",
    "                page_content = doc[content_key]\n",
    "            ) for doc in data\n",
    "        ]\n",
    "\n",
    "        id_list = [doc[pk] for doc in data]\n",
    "\n",
    "        for i in tqdm(range(0, len(document_list), batch_size)):\n",
    "            batch = document_list[i:i+batch_size]\n",
    "            batch_ids = id_list[i:i+batch_size]\n",
    "            self.retreivers[collection_name].add_documents(documents=batch, ids=batch_ids)\n",
    "    \n",
    "    def retrieve(self, collection_name:str, query:str, top_k:int=10):\n",
    "        return self.retreivers[collection_name].invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HANDLER = QdrantHandler(QCLIENT, sparse_enocder=lambda x: tuple(splade_model.encode_text(x).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection sample_collection is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/w9d_8wb17m50mtkv2w368bqc0000gn/T/ipykernel_30487/701227903.py:28: LangChainDeprecationWarning: The class `QdrantSparseVectorRetriever` was deprecated in LangChain 0.2.16 and will be removed in 0.5.0. Use :meth:`~Qdrant vector store now supports sparse retrievals natively. Use langchain_qdrant.QdrantVectorStore#as_retriever() instead. Reference: https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#sparse-vector-search` instead.\n",
      "  retriever = QdrantSparseVectorRetriever(\n"
     ]
    }
   ],
   "source": [
    "QDRANT_HANDLER.init_collections([\"sample_collection\"], reset_before_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDRANT_HANDLER.add_documents(\"sample_collection\", list(sample_json_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = QDRANT_HANDLER.retrieve(\"sample_collection\", \"What is nerural network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A neural network consists of layers of interconnected nodes. Each node in a layer processes an input, applies a weight and an activation function, and passes the result to the next layer. The final output layer generates predictions based on the input data.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,ToolMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from together import Together\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_generator(query: str, contexts: List[str], history: List[Any]):\n",
    "    # Define the system prompt for concise question answering\n",
    "    system_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a template with placeholders for system prompt, query, context, and history\n",
    "    template = \"\"\"\n",
    "    System Instruction:\n",
    "    {system_prompt}\n",
    "\n",
    "    Given the following context:\n",
    "    {contexts}\n",
    "\n",
    "    And the previous conversation history:\n",
    "    {history}\n",
    "\n",
    "    Answer the following query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the inputs into a prompt template using Langchain's PromptTemplate class\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"contexts\", \"history\", \"query\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    history_text = \"\\n\".join([f\"{'Human' if isinstance(item, HumanMessage) else 'System'}: {item.content}\" for item in history]) \n",
    "\n",
    "    # Format the prompt with the actual values\n",
    "    formatted_prompt = prompt.format(\n",
    "        system_prompt=system_prompt.strip(),\n",
    "        contexts=\"\\n\".join(contexts),\n",
    "        history=history_text,\n",
    "        query=query\n",
    "    )\n",
    "    \n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEngine:\n",
    "    def __init__(self, model_path:str=\"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\"):\n",
    "        self.client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))\n",
    "        self.model_path = model_path\n",
    "    \n",
    "    def invoke(self, query:str, contexts:List[str], history:List[Any]):\n",
    "        prompt=prompt_generator(\n",
    "                query=query,\n",
    "                contexts=contexts,\n",
    "                history=history\n",
    "                )\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_path,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ENGINE = LLMEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    cur_state: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "    contexts: List[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentConfig(TypedDict):\n",
    "    qdrant_handler: QdrantHandler = QDRANT_HANDLER\n",
    "    sample_collection: str = \"sample_collection\"\n",
    "\n",
    "class AgentFunctions:\n",
    "    @staticmethod\n",
    "    def welcome(state:AgentState)->AgentState:\n",
    "        msg = SystemMessage(\"Welcome to the system. How can I help you?\")\n",
    "        state['cur_state'] = \"welcome\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        state['messages'].append(msg)\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def human_input(state:AgentState)->AgentState:\n",
    "        state['cur_state'] = \"human_input\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def retreive_documents(state:AgentState)->AgentState:\n",
    "        query = state['messages'][-1].content\n",
    "        retrieved_nodes = AgentConfig.qdrant_handler.retrieve(AgentConfig.sample_collection, query)\n",
    "        state['contexts'] = [node.page_content for node in retrieved_nodes] \n",
    "        state['cur_state'] = \"retrieval\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def invoke_llm(state:AgentState)->AgentState:\n",
    "        query = state['messages'][-1].content\n",
    "        history = state['messages'][-4:-1]\n",
    "        contexts = state['contexts']\n",
    "        response = LLM_ENGINE.invoke(query, contexts, history)\n",
    "        msg = SystemMessage(response)\n",
    "        state['messages'].append(msg)\n",
    "        state['cur_state'] = \"invoke_llm\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_end_of_conversation(state:AgentState)->bool:\n",
    "        state['cur_state'] = \"check_end_of_conversation\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        if state['messages'][-1].content == \"/quit\":\n",
    "            return \"quit\"\n",
    "        return \"continue\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def exit_conversation(state:AgentState)->AgentState:\n",
    "        state['cur_state'] = \"exit_conversation\"\n",
    "        print(f\"Current State: {state['cur_state']}\")\n",
    "        msg = SystemMessage(\"Goodbye!\")\n",
    "        state['messages'].append(msg)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent:\n",
    "    def __init__(self,thread_id):\n",
    "        self.config = None\n",
    "        self.app = None\n",
    "        self.build(thread_id)\n",
    "\n",
    "    def draw_workflow(self):\n",
    "        display(Image(self.app.get_graph(xray=True).draw_mermaid_png()))\n",
    "    \n",
    "    def build(self, thread_id):\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"welcome\", AgentFunctions.welcome)\n",
    "        workflow.add_node(\"human_input\", AgentFunctions.human_input)\n",
    "        workflow.add_node(\"retreive_documents\", AgentFunctions.retreive_documents)\n",
    "        workflow.add_node(\"invoke_llm\", AgentFunctions.invoke_llm)\n",
    "        workflow.add_node(\"exit_conversation\", AgentFunctions.exit_conversation)\n",
    "\n",
    "        \n",
    "        workflow.add_edge(\"welcome\", \"human_input\")\n",
    "        # workflow.add_conditional_edges(\n",
    "        #     \"human_input\", \n",
    "        #     AgentFunctions.check_end_of_conversation,\n",
    "        #     {\n",
    "        #         \"quit\": \"exit_conversation\",\n",
    "        #         \"continue\": \"retreive_documents\"\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        workflow.add_edge(\"human_input\", \"retreive_documents\")\n",
    "        workflow.add_edge(\"retreive_documents\", \"invoke_llm\")\n",
    "        workflow.add_edge(\"invoke_llm\", \"exit_conversation\")\n",
    "        workflow.add_edge(\"exit_conversation\", langgraph.graph.END)\n",
    "\n",
    "        workflow.set_entry_point(\"welcome\")\n",
    "        \n",
    "        memory = MemorySaver()\n",
    "\n",
    "        app = workflow.compile(\n",
    "            checkpointer=memory,\n",
    "            interrupt_before=[\"human_input\"],\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Assign the compiled app before the connection closes\n",
    "        self.app = app\n",
    "        self.config = {\"configurable\": {\"thread_id\": str(thread_id)}}\n",
    "\n",
    "    def get_recent_state_snap(self):\n",
    "        return self.app.get_state(config=self.config).values.copy()\n",
    "\n",
    "    def get_last_message(self):\n",
    "        snap = self.get_recent_state_snap()\n",
    "        return snap[\"messages\"][-1]\n",
    "    \n",
    "    def continue_flow(self, state):\n",
    "        self.app.invoke(state, config=self.config)\n",
    "        return self.get_recent_state_snap()\n",
    "    \n",
    "    def resume_with_user_input(self, user_input:str):\n",
    "        snap = self.get_recent_state_snap()\n",
    "        snap[\"messages\"].append(HumanMessage(user_input))\n",
    "        self.app.update_state(self.config, snap)\n",
    "        return self.continue_flow(snap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_agent = MyAgent(thread_id=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_agent.draw_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: welcome\n"
     ]
    }
   ],
   "source": [
    "sample_snap0 =  sample_agent.continue_flow({\n",
    "    \"cur_state\": \"start\",\n",
    "    \"messages\": [],\n",
    "    \"contexts\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Welcome to the system. How can I help you?', additional_kwargs={}, response_metadata={}, id='f28fe5d8-04b0-4782-b81f-6c694142b3aa')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_snap0['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the system. How can I help you?\n"
     ]
    }
   ],
   "source": [
    "print(sample_agent.get_last_message().content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cur_state': 'welcome',\n",
       " 'messages': [SystemMessage(content='Welcome to the system. How can I help you?', additional_kwargs={}, response_metadata={}, id='a0cd0610-d334-4a55-84b7-3aef04da3de2')],\n",
       " 'contexts': []}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_agent.get_recent_state_snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: welcome\n"
     ]
    }
   ],
   "source": [
    "sample_snap1 = sample_agent.resume_with_user_input(\"Hi! I want to know about neural networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

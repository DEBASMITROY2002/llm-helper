{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, lowe_case=False, \n",
    "                 lemmatization=False, \n",
    "                 stem=False, \n",
    "                 stop_word_removal=False, \n",
    "                 split_pattern=r'[ ,.]', \n",
    "                 white_space_replace_pattern=r'\\s+'):\n",
    "        \n",
    "        self.lowe_case = lowe_case\n",
    "        self.lemmatization = lemmatization\n",
    "        self.stem = stem\n",
    "        self.stop_word_removal = stop_word_removal\n",
    "\n",
    "        self.split_pattern = split_pattern\n",
    "        self.white_space_replace_pattern = white_space_replace_pattern\n",
    "\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def remove_white_space(self, text: str) -> str:\n",
    "        return re.sub(self.white_space_replace_pattern, ' ', text)\n",
    "    \n",
    "    def lemmatize(self, words: List[str]) -> List[str]:\n",
    "        return [self.lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    def stemm(self, words: List[str]) -> List[str]:\n",
    "        return [self.stemmer.stem(word) for word in words]\n",
    "\n",
    "    def remove_stop_words(self, words: List[str]) -> List[str]:\n",
    "        return [word for word in words if word not in self.stop_words]\n",
    "\n",
    "    def process_text(self, text: str) -> List[str]:\n",
    "        if self.lowe_case:\n",
    "            text = text.lower()\n",
    "        words = re.split(self.split_pattern, text)\n",
    "        if self.lemmatization:\n",
    "            words = self.lemmatize(words)\n",
    "        if self.stem:\n",
    "            words = self.stemm(words)\n",
    "        if self.stop_word_removal:\n",
    "            words = self.remove_stop_words(words)\n",
    "        \n",
    "        processed_text = ' '.join(words)\n",
    "        processed_text =  self.remove_white_space(processed_text)\n",
    "        processed_text =  self.remove_white_space(processed_text) # remove white space again\n",
    "        return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_docs(paragraph, k=10, o=3, split_pattern=r'[ ,.]'):\n",
    "    \"\"\"\n",
    "    Splits a paragraph into segments of length k with an overlap of o words.\n",
    "    \n",
    "    Parameters:\n",
    "    - paragraph (str): The long paragraph to split.\n",
    "    - k (int): The length of each segment in words.\n",
    "    - o (int): The number of overlapping words between consecutive segments.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of paragraph segments.\n",
    "    \"\"\"\n",
    "    words = re.split(split_pattern, paragraph)\n",
    "    segments = []\n",
    "    \n",
    "    # Loop through words in steps of k - o to create overlapping segments\n",
    "    for i in range(0, len(words), k - o):\n",
    "        segment = \" \".join(words[i:i + k])\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # Stop if the next starting index goes beyond the list length\n",
    "        if i + k >= len(words):\n",
    "            break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, lowe_case=False, \n",
    "                 lemmatization=False, \n",
    "                 stem=False, \n",
    "                 stop_word_removal=False, \n",
    "                 split_pattern=r'[ ,.]', \n",
    "                 white_space_replace_pattern=r'\\s+|[.,!@#$%^&*()_+={}\\[\\]:;\"\\'|<>,.?/~`\\\\-]',\n",
    "                 chunk_mode = False,\n",
    "                 chunk_size=10,\n",
    "                 overlap=3):\n",
    "        self.text_processor = TextProcessor(lowe_case, \n",
    "                                            lemmatization, \n",
    "                                            stem, \n",
    "                                            stop_word_removal, \n",
    "                                            split_pattern, \n",
    "                                            white_space_replace_pattern)\n",
    "        self.chunk_mode = chunk_mode\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def create_chunk_metadata(self, actual_text: str, preprocessed_text: str , chunk_id: int):\n",
    "        return {\n",
    "            'actual_text': actual_text,\n",
    "            'preprocessed_text': preprocessed_text,\n",
    "            'chunk_id': chunk_id\n",
    "        }\n",
    "\n",
    "    def preprocess(self, text_metatdata: str) -> List[str]:\n",
    "        \"\"\" Preprocess a single text_metatdata. Returns a list of preprocessed texts \"\"\"\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        text_metatdata: {\n",
    "            'text': str,\n",
    "            'doc_id': str\n",
    "        }\n",
    "        \"\"\"\n",
    "        # processed_text = self.text_processor.process_text(text)\n",
    "        # if not self.chunk_mode:\n",
    "        #     return [processed_text]\n",
    "        # chunks = chunk_docs(processed_text, self.chunk_size, self.overlap, self.text_processor.split_pattern)\n",
    "\n",
    "\n",
    "        if not self.chunk_mode:\n",
    "            return [self.create_chunk_metadata(actual_text = self.text_processor.process_text(text_metatdata['text']), \n",
    "                                               preprocessed_text = text_metatdata['text'], \n",
    "                                               chunk_id = text_metatdata['text']+\"_\"+\"0\")\n",
    "                                               ]\n",
    "        \n",
    "        chunks = chunk_docs(text_metatdata['text'], self.chunk_size, self.overlap, self.text_processor.split_pattern)\n",
    "\n",
    "        chunks = [self.create_chunk_metadata(actual_text = chunk,\n",
    "                                             preprocessed_text = self.text_processor.process_text(chunk),\n",
    "                                             chunk_id = text_metatdata['doc_id']+\"_\"+str(i)) for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def preprocess_all(self, texts: List[Dict]) -> List[List[str]]:\n",
    "        \"\"\" Preprocess a list of texts. Returns a list of preprocessed texts \"\"\"\n",
    "        outputs = []\n",
    "        for text in texts:\n",
    "            outputs.extend(self.preprocess(text))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "    \"Concurrent lab workup indicates hypoxemia with a PaO2 of 55 mmHg, mild hypercapnia (PaCO2 of 47 mmHg), and a pH of 7.35 suggestive of compensated respiratory acidosis. Pulmonary function tests (PFTs) demonstrate a decreased FEV1/FVC ratio, confirming obstructive ventilatory impairment. The patient’s peak expiratory flow (PEF) remains under 70% of predicted values, correlating with symptomatic bronchospasm.\",\n",
    "    \n",
    "    \"The patient, a 67-year-old male with a BMI of 32 kg/m², presents with a 12-year history of HTN, T2DM (HbA1c 8.9%), and a recent NSTEMI managed with PCI to the LAD, complicated by LVEF of 38%. Baseline labs show elevated CRP (12 mg/L), BUN (24 mg/dL), and serum Cr of 1.5 mg/dL (eGFR of 52 mL/min/1.73m²). EKG reveals a QRS duration of 132 ms with LBBB morphology, while a TTE indicates moderate MR and dilated LV with an end-diastolic diameter of 6.5 cm. PFTs confirm moderate obstructive lung disease with FEV1 at 60% predicted and an FEV1/FVC ratio of 0.58, with DLCO reduced to 65% of predicted, suggestive of concurrent emphysema secondary to a 45-pack-year smoking history. The patient is currently on dual antiplatelet therapy (DAPT) with ASA 81 mg and clopidogrel 75 mg, high-intensity atorvastatin (80 mg), and ACEi titrated to target dose; however, BP remains uncontrolled (average 158/92 mmHg) despite the addition of HCTZ 25 mg and amlodipine 10 mg daily.\",\n",
    "    \n",
    "    \"The 55-year-old female with a BMI of 28 kg/m² reports progressive dyspnea and orthopnea. BNP levels are elevated at 520 pg/mL, with a serum Na+ of 133 mEq/L and K+ of 5.1 mEq/L. Echocardiogram reveals an LVEF of 25%, along with grade III diastolic dysfunction and severe MR. Coronary angiography identifies significant stenosis in the RCA, requiring DES placement. Pulmonary function tests (PFTs) show a TLC of 80% predicted, with a DLCO at 55%, consistent with restrictive physiology.\",\n",
    "    \n",
    "    \"Patient, a 72-year-old male with known COPD (GOLD stage III) and an FEV1/FVC ratio of 0.52, presents with worsening SOB and peripheral edema. ABG on room air shows pH 7.33, PaCO2 of 60 mmHg, and PaO2 of 62 mmHg, indicating acute-on-chronic hypercapnic respiratory failure. Chest CT reveals bilateral basilar opacities consistent with chronic interstitial changes. Labs notable for elevated BNP (750 pg/mL) and serum Cr 1.3 mg/dL (eGFR of 58 mL/min/1.73m²).\",\n",
    "    \n",
    "    \"A 49-year-old male presents post-syncope with elevated troponin I (0.6 ng/mL) and CK-MB at 15 U/L. EKG shows T-wave inversions in leads V2-V4 and LBBB. Cath reveals a 95% stenosis in the proximal LAD, managed with PCI and BMS. Blood work reveals Hb of 13.5 g/dL, platelets at 250,000/µL, and LFTs showing AST/ALT of 56/63 U/L, suggesting mild hepatocellular injury. The patient is started on beta-blockers, ACEi, and ASA.\",\n",
    "    \n",
    "    \"The 60-year-old male with a 30-pack-year smoking history presents with exertional angina and a recent NSTEMI. Post-PCI with stenting to RCA, ECHO shows an EF of 45%, and LGE on CMR suggests nonviable myocardium. LDL remains elevated at 132 mg/dL despite atorvastatin 40 mg. Labs reveal CRP 14 mg/L and fibrinogen of 480 mg/dL, indicative of systemic inflammation. Dual antiplatelet therapy initiated, with target BP <130/80 mmHg using losartan and thiazide.\",\n",
    "    \n",
    "    \"An 82-year-old female with CKD stage 3 (eGFR 48 mL/min/1.73m²) and longstanding atrial fibrillation presents with a baseline INR of 2.8 on warfarin. TTE shows LA enlargement with mild MR and TR. Recent labs reveal HbA1c of 7.8%, serum Cr at 1.4 mg/dL, and BNP of 480 pg/mL. PFTs reveal FVC at 75% of predicted and FEV1/FVC of 0.65, consistent with obstructive ventilatory defect secondary to COPD.\",\n",
    "    \n",
    "    \"The patient, a 68-year-old with T2DM and HTN, presents with peripheral neuropathy and nephropathy. A1C is elevated at 9.2%, while urine microalbumin is 230 mg/g. Serum Na+ is 136 mEq/L, K+ at 4.6 mEq/L, and fasting lipid panel shows LDL 145 mg/dL, HDL 38 mg/dL. Diabetic retinopathy is confirmed by fundoscopy, showing microaneurysms and hemorrhages. Treatment plan includes SGLT2 inhibitor addition and ACE inhibitor titration.\",\n",
    "    \n",
    "    \"75-year-old male with HFrEF (EF 35%) and NYHA Class III symptoms reports increased exertional fatigue. Baseline labs show BUN 29 mg/dL, serum Cr 1.6 mg/dL, and NT-proBNP at 1400 pg/mL. TTE indicates moderate MR and severe TR with an RVSP of 45 mmHg. CXR reveals cardiomegaly and pulmonary vascular congestion. The patient is on sacubitril/valsartan, metoprolol succinate, and spironolactone.\",\n",
    "    \n",
    "    \"53-year-old female with a BMI of 31 kg/m² and history of recurrent DVT presents with left leg swelling. Doppler US shows acute proximal DVT. Labs notable for elevated D-dimer (5.8 µg/mL), INR of 1.2, and PLT count of 330,000/µL. Hypercoagulable workup pending, though heterozygous Factor V Leiden mutation was previously identified. Anticoagulation initiated with LMWH bridging to warfarin.\"\n",
    "]\n",
    "\n",
    "sample_docs_metadata = [\n",
    "    {\"text\": doc, \"doc_id\": str(i)} for i, doc in enumerate(sample_docs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(lowe_case=True,\n",
    "                            lemmatization=True,\n",
    "                            stop_word_removal=True,\n",
    "                            split_pattern=r'[ ,.]',\n",
    "                            white_space_replace_pattern=r'\\s+|d+|[.,!@#$%^&*()_+={}\\[\\]:;\"\\'|<>,.?/~`\\\\-]',\n",
    "                            chunk_mode=True,\n",
    "                            chunk_size=32,\n",
    "                            overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = preprocessor.preprocess_all(sample_docs_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "{'actual_text': 'impairment  The patient’s peak expiratory flow (PEF) remains under 70% of predicted values  correlating with symptomatic bronchospasm ', 'preprocessed_text': 'impairment patient’s peak expiratory flow pef remains 70 pre icte value correlating symptomatic bronchospasm ', 'chunk_id': '0_2'}\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_docs))\n",
    "print(preprocessed_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way 1 : Shingling And Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shingler:\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "        self.vocab = None\n",
    "\n",
    "    def _shingle_document(self, doc):\n",
    "        \"\"\"Generate k-shingles for a single document (private method).\"\"\"\n",
    "        shingles = set()\n",
    "        for i in range(len(doc) - self.k + 1):\n",
    "            shingles.add(doc[i:i + self.k])\n",
    "        return shingles\n",
    "\n",
    "    def _create_vocab(self, shingles_list):\n",
    "        \"\"\"Create a vocabulary from a list of shingle sets (private method).\"\"\"\n",
    "        vocab = shingles_list[0]\n",
    "        for shingles in shingles_list[1:]:\n",
    "            vocab = vocab.union(shingles)\n",
    "        return list(vocab)  # Convert to list for consistent ordering\n",
    "\n",
    "    def _create_1hot_encoding(self, shingles):\n",
    "        \"\"\"Create a one-hot encoding vector for a single set of shingles.\"\"\"\n",
    "        return [1 if v in shingles else 0 for v in self.vocab]\n",
    "\n",
    "    def build(self, docs):\n",
    "\n",
    "        \"\"\"Shingles documents, creates a vocabulary, and produces one-hot encodings.\"\"\"\n",
    "        # Step 1: Shingle each document\n",
    "        shingles_list = [self._shingle_document(doc) for doc in docs]\n",
    "\n",
    "        # Step 2: Create the vocabulary from shingles\n",
    "        self.vocab = self._create_vocab(shingles_list)\n",
    "\n",
    "        # Step 3: Generate one-hot encodings for each document\n",
    "        one_hot_encodings = [self._create_1hot_encoding(shingles) for shingles in shingles_list]\n",
    "\n",
    "        return one_hot_encodings\n",
    "    \n",
    "    def convert_text_to_one_hot(self, text:str):\n",
    "        shingles = self._shingle_document(text)\n",
    "        return self._create_1hot_encoding(shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LshSimilaritySearch:\n",
    "    def __init__(self, documents: List[str], k=2, num_bits=256):\n",
    "        self.original_documents = documents\n",
    "        self.documents = [doc['preprocessed_text'] for doc in documents]\n",
    "        self.shingler = Shingler(k=k)\n",
    "        self.one_hot_encodings = self.shingler.build(self.documents)\n",
    "        self.one_hot_encodings_numpy = np.array(self.one_hot_encodings)\n",
    "        self.d = self.one_hot_encodings_numpy.shape[1]\n",
    "        self.num_bits = num_bits\n",
    "        self.index = faiss.IndexLSH(self.d, self.num_bits)\n",
    "        self.index.add(self.one_hot_encodings_numpy)\n",
    "\n",
    "    def search(self, query: str, top_k=5):\n",
    "        query_one_hot = self.shingler.convert_text_to_one_hot(query)\n",
    "        query_one_hot_numpy = np.array(query_one_hot).reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_one_hot_numpy, top_k)\n",
    "        results = []\n",
    "        for i in indices[0]:\n",
    "            if i != -1:\n",
    "                results.append(self.original_documents[i])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_similarity_search = LshSimilaritySearch(preprocessed_docs, k=3, num_bits=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Sample AIC value for peripheral neuropathy \"\n",
    "results = lsh_similarity_search.search(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 7_0 - Actual Text: The patient  a 68-year-old with T2DM and HTN  presents with peripheral neuropathy and nephropathy  A1C is elevated at 9 2%  while urine microalbumin is 230 mg/g  Serum\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document ID: 9_0 - Actual Text: 53-year-old female with a BMI of 31 kg/m² and history of recurrent DVT presents with left leg swelling  Doppler US shows acute proximal DVT  Labs notable for elevated D-dimer (5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document ID: 6_0 - Actual Text: An 82-year-old female with CKD stage 3 (eGFR 48 mL/min/1 73m²) and longstanding atrial fibrillation presents with a baseline INR of 2 8 on warfarin  TTE shows LA enlargement with mild\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document ID: 3_2 - Actual Text: 62 mmHg  indicating acute-on-chronic hypercapnic respiratory failure  Chest CT reveals bilateral basilar opacities consistent with chronic interstitial changes  Labs notable for elevated BNP (750 pg/mL) and serum Cr 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document ID: 0_2 - Actual Text: impairment  The patient’s peak expiratory flow (PEF) remains under 70% of predicted values  correlating with symptomatic bronchospasm \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"Document ID: {result['chunk_id']} - Actual Text: {result['actual_text']}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permuatator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnkChecker:\n",
    "    def __init__(self, dict_path=None):\n",
    "        self.dict_path = dict_path\n",
    "        with open(dict_path, \"r\") as f:\n",
    "            self.dictionary = set(f.read().splitlines())\n",
    "    \n",
    "    def check(self, word: str) -> bool:\n",
    "        \"\"\" Check if a word is in the dictionary. True if the word is in the dictionary, False otherwise. \"\"\"\n",
    "        check_flag = word in self.dictionary\n",
    "        return check_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_CHECKER = UnkChecker(dict_path=\"../dictionary/dict_v0_processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_word(word: str, shingle_length=3):\n",
    "    \"\"\" Creates some permutation of a word \"\"\"\n",
    "    if len(word) <= shingle_length:\n",
    "        return [word]\n",
    "    permutations = []\n",
    "    for i in range(len(word) - shingle_length + 1):\n",
    "        permutations.append(word[i:i + shingle_length])\n",
    "    return permutations\n",
    "\n",
    "def permutate_para(text: str, split_pattern=r'[ ,.]', long_word_length=6, shingle_length=3):\n",
    "    \n",
    "    assert long_word_length is not None, \"long_word_length must be provided\"\n",
    "    assert shingle_length is not None, \"shingle_length must be provided\"\n",
    "\n",
    "    \"\"\" Creates some permutation of each long and unknown words in the text \"\"\"\n",
    "    outout_words = []\n",
    "    all_words = re.split(split_pattern, text)\n",
    "    for word in all_words:\n",
    "        if len(word) > long_word_length and not UNK_CHECKER.check(word):\n",
    "            outout_words.extend(word)\n",
    "            outout_words.extend(permutate_word(word))\n",
    "        else:\n",
    "            outout_words.append(word)\n",
    "    \n",
    "    output_text = ' '.join(outout_words)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfSimilaritySearch:\n",
    "    def __init__(self, preprocessed_documents, permutate_mode=False, long_word_length=None, shingle_length=None):\n",
    "        self.original_documents = preprocessed_documents\n",
    "        self.permutate_mode = permutate_mode\n",
    "        self.long_word_length = long_word_length\n",
    "        self.shingle_length = shingle_length\n",
    "\n",
    "        self.documents = [doc['preprocessed_text'] for doc in self.original_documents]\n",
    "        if self.permutate_mode:\n",
    "            self.documents = [permutate_para(doc, \n",
    "                                             long_word_length=self.long_word_length,\n",
    "                                             shingle_length=self.shingle_length) for doc in self.documents]\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "        self.preprocessor = Preprocessor(lowe_case=True,\n",
    "                                         lemmatization=True,\n",
    "                                         stop_word_removal=True,\n",
    "                                         split_pattern=r'[ ,.]',\n",
    "                                         white_space_replace_pattern=r'\\s+|d+|[.,!@#$%^&*()_+={}\\[\\]:;\"\\'|<>,.?/~`\\\\-]',\n",
    "                                         chunk_mode=False)\n",
    "\n",
    "    def search(self, query, top_n=3, query_permute_mode=True):\n",
    "        # Process the query\n",
    "        query = self.preprocessor.preprocess({'text': query, 'doc_id': 'query'})[0]['preprocessed_text']\n",
    "\n",
    "        if query_permute_mode:\n",
    "         # Permuate long and unk words in the query\n",
    "            query = permutate_para(query, long_word_length=self.long_word_length, shingle_length=self.shingle_length)\n",
    "\n",
    "        # Transform the query into TF-IDF vector\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        # Calculate cosine similarities between the query and documents\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()\n",
    "        # Get top_n most similar documents\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        \n",
    "        return [(self.original_documents[i], similarities[i]) for i in top_indices if similarities[i] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_search = TfidfSimilaritySearch(preprocessed_docs, permutate_mode=True, long_word_length=6, shingle_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Sample AIC value for peripheral neuropathy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = tfidf_search.search(query, top_n=5, query_permute_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.12): impairment  The patient’s peak expiratory flow (PEF) remains under 70% of predicted values  correlating with symptomatic bronchospasm  from doc_id: 0_2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.12): Patient  a 72-year-old male with known COPD (GOLD stage III) and an FEV1/FVC ratio of 0 52  presents with worsening SOB and peripheral edema  ABG on room air shows from doc_id: 3_0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.11): SOB and peripheral edema  ABG on room air shows pH 7 33  PaCO2 of 60 mmHg  and PaO2 of 62 mmHg  indicating acute-on-chronic hypercapnic respiratory failure  Chest from doc_id: 3_1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.09): The patient  a 68-year-old with T2DM and HTN  presents with peripheral neuropathy and nephropathy  A1C is elevated at 9 2%  while urine microalbumin is 230 mg/g  Serum from doc_id: 7_0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (doc, sim) in enumerate(top_results):\n",
    "    print(f\"Result {i+1} (Similarity: {sim:.2f}): {doc['actual_text']} from doc_id: {doc['chunk_id']}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25SimilaritySearch:\n",
    "    def __init__(self, documents, permutate_mode=False, long_word_length=None, shingle_length=None):\n",
    "        self.original_documents = documents\n",
    "        self.permutate_mode = permutate_mode\n",
    "        self.long_word_length = long_word_length\n",
    "        self.shingle_length = shingle_length\n",
    "        \n",
    "        # Apply shingling if needed\n",
    "        self.documents = [doc['preprocessed_text'] for doc in documents]\n",
    "\n",
    "        \n",
    "        if permutate_mode:\n",
    "            self.documents = [permutate_para(doc, \n",
    "                                             long_word_length=long_word_length,\n",
    "                                             shingle_length=shingle_length) for doc in self.documents]\n",
    "\n",
    "        # Tokenize documents\n",
    "        self.tokenized_docs = [self._tokenize(doc) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        self.preprocessor = Preprocessor(\n",
    "            lowe_case=True,\n",
    "            lemmatization=True,\n",
    "            stop_word_removal=True,\n",
    "            split_pattern=r'[ ,.]',\n",
    "            white_space_replace_pattern=r'\\s+|d+|[.,!@#$%^&*()_+={}\\[\\]:;\"\\'|<>,.?/~`\\\\-]',\n",
    "            chunk_mode=False\n",
    "        )\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return re.split(r'\\s+', text)\n",
    "\n",
    "    def search(self, query, top_n=3, query_permute_mode=False):\n",
    "        # Preprocess the query\n",
    "        query = self.preprocessor.preprocess({'text': query, 'doc_id': 'query'})[0]['preprocessed_text']\n",
    "\n",
    "        if query_permute_mode:\n",
    "            query = permutate_para(query, long_word_length=self.long_word_length, shingle_length=self.shingle_length)\n",
    "\n",
    "        # Tokenize the query\n",
    "        tokenized_query = self._tokenize(query)\n",
    "        # Get BM25 scores\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        # Get top_n results\n",
    "        top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "        \n",
    "        return [(self.original_documents[i], scores[i]) for i in top_indices if scores[i] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_search = BM25SimilaritySearch(preprocessed_docs, permutate_mode=True, long_word_length=6, shingle_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Sample AIC value for peripheral neuropathy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = bm25_search.search(query, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 3.01): Patient  a 72-year-old male with known COPD (GOLD stage III) and an FEV1/FVC ratio of 0 52  presents with worsening SOB and peripheral edema  ABG on room air shows from doc_id: 3_0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 2.68): SOB and peripheral edema  ABG on room air shows pH 7 33  PaCO2 of 60 mmHg  and PaO2 of 62 mmHg  indicating acute-on-chronic hypercapnic respiratory failure  Chest from doc_id: 3_1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 2.62): impairment  The patient’s peak expiratory flow (PEF) remains under 70% of predicted values  correlating with symptomatic bronchospasm  from doc_id: 0_2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 2.33): The patient  a 68-year-old with T2DM and HTN  presents with peripheral neuropathy and nephropathy  A1C is elevated at 9 2%  while urine microalbumin is 230 mg/g  Serum from doc_id: 7_0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (doc, sim) in enumerate(top_results):\n",
    "    print(f\"Result {i+1} (Similarity: {sim:.2f}): {doc['actual_text']} from doc_id: {doc['chunk_id']}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
